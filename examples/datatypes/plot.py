#!/usr/bin/env python3

import csv
import argparse
import re
import numpy as np
import pandas as pd
from math import ceil

from os import listdir
from os.path import isfile, join

parser = argparse.ArgumentParser(
    prog='plot.py',
    description='Plot data generated by the datatypes benchmark',
    epilog='Report bugs to Pengcheng Xu <pengxu@ethz.ch>.'
)

parser.add_argument('--data_root', help='root of the CSV files from the datatypes benchmark', default=None)

args = parser.parse_args()

def dim_to_gflops(dim, iter, elapsed):
    return 2 * dim ** 3 * iter / elapsed / 1e9
def rtt_to_mbps(sbuf_sz, num_parallel, rtt):
    return sbuf_sz * 8 * num_parallel / rtt / 1e6
def cycles_to_us(cycles):
    return 1e6 / pspin_freq * cycles

slmp_payload_size = 1462 // 4 * 4
pspin_freq = 40e6 # 40 MHz
parallel_pkl = 'parallel.pkl'
msg_size_pkl = 'msg_size.pkl'

dgemm_theo_label = 'DGEMM Theoretical'
dgemm_ref_label = 'DGEMM Reference'
dgemm_overlap_label = 'DGEMM Overlap'

dt_theo_label = 'Datatypes MPICH'
dt_ref_label = 'Datatypes Reference'
dt_overlap_label = 'Datatypes Overlap'

def consume_trials(key, full_key, trials, num_parallel_func):
    # https://stackoverflow.com/a/42837693/5520728
    def append_row(num, type, value):
        global data
        entry = pd.DataFrame.from_dict({
            full_key: [num],
            'type': [type],
            'value': [value],
        })
        data = pd.concat([data, entry], ignore_index=True)

    for t in trials:
        if m := re.match(key + r'-([0-9]+)\.csv', t):
            val = m.group(1)
        else:
            continue

        num_parallel = num_parallel_func(float(val))

        with open(join(args.data_root, t), 'r') as f:
            reader = csv.reader(f)
            assert next(reader) == ['gflops_theo', 'gflops_ref', 'dim', 'streambuf_size']
            tg, rg, dim, sbuf_size = [float(x) for x in next(reader)]
            num_packets = ceil(sbuf_size / slmp_payload_size)

            append_row(val, dgemm_theo_label, tg)
            # dgemm w/o dt
            append_row(val, dgemm_ref_label, rg)
            assert next(reader) == []

            assert next(reader) == ['dgemm', 'iters', 'dt_ref_rtt',
                                    'dt_ref_pkt', 'dt_ref_msg', 'dt_rtt', 'dt_pkt', 'dt_msg']
            for data_tuple in reader:
                # dgemm w/ dt, dt w/o dgemm, dt w/ dgemm
                # RTT in float seconds
                dgemm_overlap, dgemm_iter, dt_ref_rtt, dt_ref_pkt, dt_ref_msg, dt_rtt, dt_pkt, dt_msg = map(float, data_tuple)
                dt_pkt *= num_packets
                dt_ref_pkt *= num_packets

                append_row(val, dgemm_overlap_label, dim_to_gflops(dim, dgemm_iter, dgemm_overlap))
                append_row(val, dt_ref_label, rtt_to_mbps(sbuf_size, num_parallel, dt_ref_rtt))
                append_row(val, 'dt_ref_rtt_us', dt_ref_rtt * 1e6)
                append_row(val, 'dt_ref_pkt_us', cycles_to_us(dt_ref_pkt))
                append_row(val, 'dt_ref_msg_us', cycles_to_us(dt_ref_msg))
                append_row(val, dt_overlap_label, rtt_to_mbps(sbuf_size, num_parallel, dt_rtt))
                append_row(val, 'dt_overlap_rtt_us', dt_rtt * 1e6)
                append_row(val, 'dt_overlap_pkt_us', cycles_to_us(dt_pkt))
                append_row(val, 'dt_overlap_msg_us', cycles_to_us(dt_msg))

                # use KB message sizes
                overlap_key = sbuf_size / 1000 if key == 'm' else val
                    
                append_row(overlap_key, 'overlap', dgemm_overlap / dt_rtt)

    # TODO: MPICH + vanilla Corundum data
    append_row(0, dt_theo_label, 20000) # dummy data of 20 Gbps

if args.data_root:
    trials = [f for f in listdir(args.data_root) if isfile(join(args.data_root, f))]
    data = pd.DataFrame(columns=['parallelism', 'type', 'value'])
    consume_trials('p', 'parallelism', trials, lambda x: x) # num_parallel is the key value
    data.to_pickle(parallel_pkl)

    data = pd.DataFrame(columns=['msg_size', 'type', 'value'])
    consume_trials('m', 'msg_size', trials, lambda _: 16)
    data.to_pickle(msg_size_pkl)

import altair as alt
from altair import datum

data_par = pd.read_pickle(parallel_pkl)
base = alt.Chart(data_par)

parallel_title = 'Degree of Parallelism'

# plot 1: dgemm tput & dt bw over parallelism
bw_title = 'Datatypes Bandwidth (Mbps)'
bw_base = base.transform_filter(
    (datum.type == dt_ref_label) | (datum.type == dt_overlap_label)
)
bw_mean = bw_base.mark_line().encode(
    x=alt.X('parallelism').title(''),
    y=alt.Y('mean(value)').title(bw_title).scale(alt.Scale(type='log')),
    color='type',
)
bw_err = bw_base.mark_errorbar(extent='stdev').encode(
    x=alt.X('parallelism'),
    y=alt.Y('value').title(bw_title),
    color='type'
)
bw_theo = base.mark_rule(strokeWidth=2).encode(
    y='value',
    strokeDash='type'
).transform_filter(
    (datum.type == dt_theo_label)
)

bw_chart = (bw_mean + bw_err + bw_theo).properties(width=400, height=150)

tput_base = base.transform_filter(
    (datum.type == dgemm_ref_label) | (datum.type == dgemm_overlap_label)
)
tput_title = 'DGEMM Throughput (GFLOPS)'
tput_mean = tput_base.mark_line().encode(
    x=alt.X('parallelism').title(parallel_title),
    y=alt.Y('mean(value)').title(tput_title).scale(domain=[80,250]),
    color=alt.Color('type', title=None),
)
tput_err = tput_base.mark_errorbar(extent='stdev').encode(
    x=alt.X('parallelism'),
    y=alt.Y('value').title(tput_title),
    color='type'
)
tput_theo = base.mark_rule(strokeWidth=2).encode(
    y=alt.Y('mean(value)'),
    strokeDash=alt.StrokeDash('type', title=None),
).transform_filter(
    (datum.type == dgemm_theo_label)
)

tput_chart = (tput_mean + tput_err + tput_theo).properties(width=400, height=150)

'''
# double y axis
alt.layer(bw_chart, tput_chart).resolve_scale(
    y='independent'
).properties(width=400, height=200).save('plot1.json')
'''
# vertically concatenated
alt.vconcat(bw_chart, tput_chart).resolve_scale(
    x='shared'
).save('plot1.json')

# plot overlapping ratio
data_msg = pd.read_pickle(msg_size_pkl)
base = alt.Chart(data_msg)

overlap_title = 'Overlap'
overlap_base = base.transform_filter(
    (datum.type == 'overlap')
)
overlap_avg = overlap_base.mark_line().encode(
    x=alt.X('msg_size').title('Message Size (KB)'),
    y=alt.Y('mean(value)', axis=alt.Axis(format='.0%')).title(overlap_title).scale(domain=[.6, 1]),
)
overlap_err = overlap_base.mark_errorbar(extent='stdev').encode(
    x=alt.X('msg_size'),
    y=alt.Y('value').title(overlap_title),
)

(overlap_avg + overlap_err).properties(width=400, height=200).save('plot2.json')

base = alt.Chart(data_par)

overlap_title = 'Overlap'
overlap_base = base.transform_filter(
    (datum.type == 'overlap')
)
overlap_avg = overlap_base.mark_line().encode(
    x=alt.X('parallelism').title(parallel_title),
    y=alt.Y('mean(value)', axis=alt.Axis(format='.0%')).title(overlap_title).scale(domain=[.6, 1]),
)
overlap_err = overlap_base.mark_errorbar(extent='stdev').encode(
    x=alt.X('parallelism'),
    y=alt.Y('value').title(overlap_title),
)

(overlap_avg + overlap_err).properties(width=400, height=200).save('plot2-1.json')